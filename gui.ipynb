{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender Features shape: (1, 128)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step\n",
      "Emotion Features shape: (1, 13, 1, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 576ms/step\n",
      "Gender Features shape: (1, 128)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load emotion model\n",
    "with open('model1.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "emotion_model.load_weights('Speech_Model.h5')\n",
    "\n",
    "# Load gender model\n",
    "with open('model2.json', 'r') as json_file:\n",
    "    loaded_gender_model_json = json_file.read()\n",
    "gender_model = model_from_json(loaded_gender_model_json)\n",
    "gender_model.load_weights('Gender_Model.h5')\n",
    "\n",
    "# Print model summaries to check expected input shapes\n",
    "\n",
    "# Manually define emotion classes\n",
    "emotion_classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad']\n",
    "\n",
    "# Fit emotion label encoder\n",
    "emotion_encoder = LabelEncoder()\n",
    "emotion_encoder.fit(emotion_classes)\n",
    "\n",
    "# Define gender labels\n",
    "label2int = {\"male\": 1, \"female\": 0}\n",
    "\n",
    "def preprocess_audio(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=16000)\n",
    "    y = librosa.effects.trim(y)[0]\n",
    "    y = librosa.util.normalize(y)\n",
    "    return y, sr\n",
    "\n",
    "def extract_feature(file_name, **kwargs):\n",
    "    mfcc = kwargs.get(\"mfcc\", False)\n",
    "    chroma = kwargs.get(\"chroma\", False)\n",
    "    mel = kwargs.get(\"mel\", False)\n",
    "    contrast = kwargs.get(\"contrast\", False)\n",
    "    tonnetz = kwargs.get(\"tonnetz\", False)\n",
    "    X, sample_rate = librosa.core.load(file_name)\n",
    "    if chroma or contrast:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    if contrast:\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, contrast))\n",
    "    if tonnetz:\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, tonnetz))\n",
    "    return result\n",
    "\n",
    "def extract_features(y, sr, n_mfcc=13):\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "    return mfccs_mean\n",
    "\n",
    "def extract_gender_features(y, sr, n_mfcc=128):\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "    return mfccs_mean\n",
    "\n",
    "def predict_gender(file_path):\n",
    "    y, sr = preprocess_audio(file_path)\n",
    "    features = extract_gender_features(y, sr, n_mfcc=128)\n",
    "    features = np.expand_dims(features, axis=0)  # Add batch dimension\n",
    "    print(f\"Gender Features shape: {features.shape}\")\n",
    "    predicted_prob = gender_model.predict(features)[0][0]\n",
    "    gender = \"female\" if predicted_prob < 0.5 else \"male\"\n",
    "    return gender\n",
    "\n",
    "def predict_emotion(file_path):\n",
    "    y, sr = preprocess_audio(file_path)\n",
    "    features = extract_features(y, sr, n_mfcc=13)\n",
    "    features = features.reshape(13, 1, 1)  # Reshape to match model input shape (13, 1, 1)\n",
    "    features = np.expand_dims(features, axis=0)  # Add batch dimension\n",
    "    print(f\"Emotion Features shape: {features.shape}\")\n",
    "    predicted_index = emotion_model.predict(features).argmax()\n",
    "    predicted_emotion = emotion_encoder.inverse_transform([predicted_index])[0]\n",
    "    return predicted_emotion if predicted_emotion in emotion_classes else None\n",
    "\n",
    "class EmotionDetectionApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Emotion Detection from Voice\")\n",
    "        self.create_widgets()\n",
    "\n",
    "    def create_widgets(self):\n",
    "        self.label = tk.Label(self.root, text=\"Upload a Voice Recording\")\n",
    "        self.label.pack(pady=10)\n",
    "\n",
    "        self.upload_button = tk.Button(self.root, text=\"Upload Audio\", command=self.upload_audio)\n",
    "        self.upload_button.pack(pady=5)\n",
    "\n",
    "        self.result_label = tk.Label(self.root, text=\"\")\n",
    "        self.result_label.pack(pady=10)\n",
    "\n",
    "    def upload_audio(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Audio Files\", \"*.wav *.mp3\")])\n",
    "        if not file_path:\n",
    "            return\n",
    "\n",
    "        gender = predict_gender(file_path)\n",
    "        if gender != \"female\":\n",
    "            self.result_label.config(text=\"Please upload a female voice\")\n",
    "            return\n",
    "\n",
    "        emotion = predict_emotion(file_path)\n",
    "        if emotion:\n",
    "            self.result_label.config(text=f\"Detected Emotion: {emotion}\")\n",
    "        else:\n",
    "            self.result_label.config(text=\"Could not detect emotion. Please upload a clear voice recording.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = EmotionDetectionApp(root)\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
